{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a06ca4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import clickhouse_connect\n",
    "\n",
    "# Show all rows\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "# Show all columns\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "# Make sure wide DataFrames don't wrap\n",
    "pd.set_option(\"display.width\", None)\n",
    "\n",
    "# Show the full content of each column (no '...')\n",
    "pd.set_option(\"display.max_colwidth\", None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d77bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = clickhouse_connect.get_client(\n",
    "    host='yaujulxk39.eastus2.azure.clickhouse.cloud',      # or server IP\n",
    "    port=8443,             # HTTP port (default)\n",
    "    username='default',\n",
    "    password='~gZjRLjjOJh1i',\n",
    "    database='Competitive_Analysis'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0601efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73699, 291)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Query ----\n",
    "df_hum = client.query_df(\"\"\"\n",
    "    SELECT *\n",
    "    FROM Competitive_Analysis.DRGHistoricalReviewInformation\n",
    "    WHERE Client = 'Hum'\n",
    "\"\"\")\n",
    "\n",
    "\"\"\"# ---- Metrics ----\n",
    "log_df = pd.DataFrame([{\n",
    "    \"Run_Timestamp\": datetime.now(),\n",
    "    \"Max_InitialDeterminationDate\": df_hum['InitialDeterminationDate'].max(),\n",
    "    \"Total_Claims\": len(df_hum),\n",
    "    \"Approved_Claims\": (df_hum['InitialDeterminationStatus'] == 'Approved').sum(),\n",
    "    \"Denied_Claims\": (df_hum['InitialDeterminationStatus'] == 'Denied').sum(),\n",
    "    \"Total_IDSavings\": df_hum['IDSavings'].sum()\n",
    "}])\n",
    "\n",
    "# ---- File info ----\n",
    "log_file = r\"C:\\Arun-MIX\\DRG Analysis Report\\DRG_DB_Query_Log.xlsx\"\n",
    "sheet_name = \"Humana\"\n",
    "\n",
    "# ---- Append safely ----\n",
    "existing_df = pd.read_excel(log_file, sheet_name=sheet_name)\n",
    "final_df = pd.concat([existing_df, log_df], ignore_index=True)\n",
    "\n",
    "with pd.ExcelWriter(\n",
    "    log_file,\n",
    "    engine=\"openpyxl\",\n",
    "    mode=\"a\",\n",
    "    if_sheet_exists=\"replace\"\n",
    ") as writer:\n",
    "    final_df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "print(\"✅ Metrics successfully appended to Excel log (Humana sheet)\")\"\"\"\n",
    "df_hum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ea9029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73699, 291)\n",
      "(73699, 291)\n",
      "<StringArray>\n",
      "['01', '10']\n",
      "Length: 2, dtype: string\n",
      "(73695, 291)\n",
      "len_adrg\n",
      "3    72660\n",
      "4     1035\n",
      "Name: count, dtype: int64\n",
      "Group_Type\n",
      "01    72660\n",
      "10     1035\n",
      "Name: count, dtype: Int64\n",
      "(73687, 292)\n",
      "Control_ID\n",
      "HUPTMR-65573    1\n",
      "HUPTMR-10204    1\n",
      "HUPTMR-49129    1\n",
      "HUPTMR-9989     1\n",
      "HUPTMR-10906    1\n",
      "Name: count, dtype: Int64\n",
      "(73687, 293)\n",
      "(73687, 293)\n",
      "(73685, 293)\n",
      "(73685, 293)\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------------------------\n",
    "# Initial shape of the dataset\n",
    "# -------------------------------------------------\n",
    "print(df_hum.shape)  \n",
    "\n",
    "df_hum = df_hum[df_hum['InitialDenialType'] == 'Substantive']\n",
    "print(df_hum.shape) \n",
    "\n",
    "# -------------------------------------------------\n",
    "# Identify valid Group_Type values\n",
    "# -------------------------------------------------\n",
    "\n",
    "group_type_unique = df_hum['Group_Type'].dropna().unique()\n",
    "print(group_type_unique)  \n",
    "\n",
    "# -------------------------------------------------\n",
    "# Filter dataset to only valid Group_Type rows\n",
    "# Use .copy() to avoid SettingWithCopyWarning\n",
    "# -------------------------------------------------\n",
    "df_hum_group_type = df_hum[df_hum['Group_Type'].isin(group_type_unique)].copy()\n",
    "print(df_hum_group_type.shape)  \n",
    "\n",
    "# -------------------------------------------------\n",
    "# Calculate ADRG length\n",
    "# -------------------------------------------------\n",
    "df_hum_group_type.loc[:, 'len_adrg'] = (\n",
    "    df_hum_group_type['ADRG'].astype(str).str.len()\n",
    ")\n",
    "\n",
    "print(df_hum_group_type['len_adrg'].value_counts())\n",
    "print(df_hum_group_type['Group_Type'].value_counts())\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Remove invalid DRG / Group_Type combinations\n",
    "#   1) Group_Type = '01' and ADRG length = 4\n",
    "#   2) Group_Type = '10' and ADRG length = 3\n",
    "# -------------------------------------------------\n",
    "mask_remove = (\n",
    "    ((df_hum_group_type['Group_Type'] == '01') & (df_hum_group_type['len_adrg'] == 4)) |\n",
    "    ((df_hum_group_type['Group_Type'] == '10') & (df_hum_group_type['len_adrg'] == 3))\n",
    ")\n",
    "\n",
    "df_hum_drg_rem = df_hum_group_type.loc[~mask_remove].copy()\n",
    "print(df_hum_drg_rem.shape)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Create a copy of InitialDeterminationDate\n",
    "# -------------------------------------------------\n",
    "df_hum_drg_rem.loc[:, 'InitialDeterminationDate_copy'] = (\n",
    "    df_hum_drg_rem['InitialDeterminationDate']\n",
    ")\n",
    "\n",
    "print(\n",
    "    df_hum_drg_rem['Control_ID']\n",
    "    .value_counts()\n",
    "    .sort_values(ascending=False)\n",
    "    .head()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Convert date column to datetime\n",
    "# -------------------------------------------------\n",
    "df_hum_drg_rem.loc[:, 'InitialDeterminationDate_copy'] = pd.to_datetime(\n",
    "    df_hum_drg_rem['InitialDeterminationDate_copy'],\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Keep latest record per Control_ID\n",
    "# -------------------------------------------------\n",
    "df_sorted = df_hum_drg_rem.sort_values(\n",
    "    'InitialDeterminationDate_copy',\n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "df_hum_latest = df_sorted.drop_duplicates(\n",
    "    subset='Control_ID',\n",
    "    keep='first'\n",
    ").copy()\n",
    "\n",
    "print(df_hum_latest.shape)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Normalize string columns\n",
    "# -------------------------------------------------\n",
    "df_hum_latest.loc[:, 'ADRG'] = (\n",
    "    df_hum_latest['ADRG'].astype(str).str.strip()\n",
    ")\n",
    "\n",
    "df_hum_latest.loc[:, 'PRIM_DX'] = (\n",
    "    df_hum_latest['PRIM_DX'].astype(str).str.strip().str.upper()\n",
    ")\n",
    "\n",
    "df_hum_latest.loc[:, 'InitialDeterminationStatus'] = (\n",
    "    df_hum_latest['InitialDeterminationStatus']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Clean numeric fields\n",
    "# -------------------------------------------------\n",
    "df_hum_latest.loc[:, 'IDSavings'] = (\n",
    "    pd.to_numeric(df_hum_latest['IDSavings'], errors='coerce').fillna(0)\n",
    ")\n",
    "\n",
    "df_hum_latest.loc[:, 'LOS'] = (\n",
    "    pd.to_numeric(df_hum_latest['LOS'], errors='coerce').fillna(0)\n",
    ")\n",
    "\n",
    "df_hum_latest.loc[:, 'AGE'] = (\n",
    "    pd.to_numeric(df_hum_latest['AGE'], errors='coerce')\n",
    ")\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Validate AGE values\n",
    "# -------------------------------------------------\n",
    "max_age = df_hum_latest['AGE'].max()\n",
    "\n",
    "df_hum_latest = df_hum_latest[\n",
    "    df_hum_latest['AGE'].between(0, max_age)\n",
    "].copy()\n",
    "\n",
    "df_hum_latest.loc[:, 'AGE'] = df_hum_latest['AGE'].astype('Int64')\n",
    "print(df_hum_latest.shape)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Validate LOS values\n",
    "# -------------------------------------------------\n",
    "df_hum_latest.loc[:, 'LOS'] = (\n",
    "    df_hum_latest['LOS'].astype('Int64')\n",
    ")\n",
    "\n",
    "df_hum_latest = df_hum_latest[df_hum_latest['LOS'] >= 0].copy()\n",
    "print(df_hum_latest.shape)\n",
    "\n",
    "# -------------------------------------------------\n",
    "# Remove extreme savings outliers\n",
    "# -------------------------------------------------\n",
    "df_hum_latest = df_hum_latest[\n",
    "    df_hum_latest['IDSavings'] < 650000\n",
    "].reset_index(drop=True)\n",
    "\n",
    "print(df_hum_latest.shape)\n",
    "\n",
    "# Date only\n",
    "df_hum_latest['InitialDeterminationDate_date'] = (\n",
    "    df_hum_latest['InitialDeterminationDate'].dt.date\n",
    ")\n",
    "\n",
    "# Year\n",
    "df_hum_latest['InitialDetermination_Year'] = (\n",
    "    df_hum_latest['InitialDeterminationDate'].dt.year\n",
    ")\n",
    "\n",
    "df_hum_latest[\"InitialDeterminationDate_date\"] = pd.to_datetime(\n",
    "    df_hum_latest[\"InitialDeterminationDate_date\"],\n",
    "    errors=\"coerce\"   # handles bad / empty values safely\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89d63862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72654, 295)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "len_adrg\n",
       "3    72654\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hum_ms_drg = df_hum_latest[df_hum_latest['Group_Type']=='01']\n",
    "print(df_hum_ms_drg.shape)\n",
    "\n",
    "df_hum_ms_drg['len_adrg'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e1846c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded MCC/CC list with 17913 entries.\n"
     ]
    }
   ],
   "source": [
    "df = df_hum_ms_drg.copy()\n",
    "\n",
    "adx_cols = [f\"A_DX{i}\" for i in range(2, 26)]\n",
    "df[\"A_DX_List\"] = df[adx_cols].values.tolist()\n",
    "df[\"A_DX_List\"] = df[\"A_DX_List\"].apply(lambda x: sorted([i for i in x if pd.notna(i)]))\n",
    "\n",
    "adx_cols = [f\"B_DX{i}\" for i in range(2, 26)]\n",
    "df[\"B_DX_List\"] = df[adx_cols].values.tolist()\n",
    "df[\"B_DX_List\"] = df[\"B_DX_List\"].apply(lambda x: sorted([i for i in x if pd.notna(i)]))\n",
    "\n",
    "\n",
    "def clean_sdx_list(lst):\n",
    "    cleaned = []\n",
    "    for item in lst:\n",
    "        item = str(item).upper().strip()               # normalize\n",
    "        item = item.replace(\"- MCC\", \"\")               # remove MCC\n",
    "        item = item.replace(\"- CC\", \"\")                # remove CC\n",
    "        item = item.replace(\"MCC\", \"\")                 # safety\n",
    "        item = item.replace(\"CC\", \"\")                  # safety\n",
    "        cleaned.append(item.strip().replace(\"-\", \"\"))  # final cleanup\n",
    "    return cleaned\n",
    "df[\"A_DX_List_Clean\"] = df[\"A_DX_List\"].apply(clean_sdx_list)\n",
    "df[\"B_DX_List_Clean\"] = df[\"B_DX_List\"].apply(clean_sdx_list)\n",
    " \n",
    "# ---------------------------\n",
    "# ------------------------------\n",
    "# Step 1: Load MCC/CC Mapping File\n",
    "# ---------------------------------------------------------\n",
    "mccandcclist_df = pd.read_excel(r\"C:\\Arun_MIX\\MCCCCList.xlsx\")\n",
    "mccandcclist_df['ICDCode'] = (\n",
    "    mccandcclist_df['ICDCode']\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    ")\n",
    "mccandcclist = dict(zip(mccandcclist_df['ICDCode'], mccandcclist_df['MCCorCC']))\n",
    "print(f\"✅ Loaded MCC/CC list with {len(mccandcclist)} entries.\")\n",
    "mccandcclist_df.head()\n",
    "\n",
    "\n",
    "def map_sdx_types(icd_list, lookup_dict):\n",
    "    cleaned_list = []\n",
    "    for code in icd_list:\n",
    "        code_clean = str(code).strip().upper()  # normalize\n",
    "\n",
    "        tag = lookup_dict.get(code_clean, \"\")  # MCC / CC / General\n",
    "        cleaned_list.append(f\"{code_clean} - {tag}\")\n",
    "\n",
    "    return cleaned_list\n",
    "\n",
    "df['A_DX_Type_list'] = df['A_DX_List_Clean'].apply(lambda x: map_sdx_types(x, mccandcclist))\n",
    "df['B_DX_Type_list'] = df['B_DX_List_Clean'].apply(lambda x: map_sdx_types(x, mccandcclist))\n",
    "\n",
    "def extract_sdx_info(sdx_list):\n",
    "\n",
    "    # If list is empty or missing\n",
    "    if not sdx_list or len(sdx_list) == 0:\n",
    "        return \"No SDX Present\", \"None\"\n",
    "\n",
    "    # Separate entries by type\n",
    "    mcc = [x.split(\" - \")[0] for x in sdx_list if \"- MCC\" in x]\n",
    "    cc  = [x.split(\" - \")[0] for x in sdx_list if \"- CC\" in x]\n",
    "    other = [\n",
    "        x.split(\" - \")[0]\n",
    "        for x in sdx_list\n",
    "        if \"- MCC\" not in x and \"- CC\" not in x\n",
    "    ]\n",
    "\n",
    "    # Determine SDX_Type and SDX_Set\n",
    "    if mcc:\n",
    "        return \"MCC\", \", \".join(mcc)\n",
    "    elif cc:\n",
    "        return \"CC\", \", \".join(cc)\n",
    "    elif other:\n",
    "        return \"General\", \"None\" #.join(other) #\"No MCC or CC\",\"None\"\n",
    "    else:\n",
    "        return  \"No SDX Present\", \"None\"\n",
    "\n",
    "\n",
    "\n",
    "# Apply the logic\n",
    "df[[\"A_DX_Type\", \"A_DX_Set\"]] = (\n",
    "    df[\"A_DX_Type_list\"]\n",
    "    .apply(lambda x: pd.Series(extract_sdx_info(x)))\n",
    ")\n",
    "\n",
    "df[[\"B_DX_Type\", \"B_DX_Set\"]] = (\n",
    "    df[\"B_DX_Type_list\"]\n",
    "    .apply(lambda x: pd.Series(extract_sdx_info(x)))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "pdx_cols = [f\"A_PX{i}\" for i in range(1, 26)]\n",
    "\n",
    "# Step 1: Collect procedure columns into a list\n",
    "df[\"A_Proc_List\"] = df[pdx_cols].values.tolist()\n",
    "\n",
    "# Step 2: Create comma-separated list or fill 'None' if empty\n",
    "df[\"A_Proc_List\"] = df[\"A_Proc_List\"].apply(\n",
    "    lambda x: (\n",
    "        \",\".join(map(str, sorted([i for i in x if pd.notna(i)])))\n",
    "        if any(pd.notna(i) for i in x)\n",
    "        else \"None\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "pdx_cols = [f\"B_PX{i}\" for i in range(1, 26)]\n",
    "\n",
    "# Step 1: Collect procedure columns into a list\n",
    "df[\"B_Proc_List\"] = df[pdx_cols].values.tolist()\n",
    "\n",
    "# Step 2: Create comma-separated list or fill 'None' if empty\n",
    "df[\"B_Proc_List\"] = df[\"B_Proc_List\"].apply(\n",
    "    lambda x: (\n",
    "        \",\".join(map(str, sorted([i for i in x if pd.notna(i)])))\n",
    "        if any(pd.notna(i) for i in x)\n",
    "        else \"None\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fed4282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dx_code(x):\n",
    "    if pd.isna(x):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and normalize\n",
    "    x = str(x).upper().strip()\n",
    "    \n",
    "    # Remove square brackets if any\n",
    "    x = x.replace(\"[\", \"\").replace(\"]\", \"\")\n",
    "    \n",
    "    # Remove MCC / CC suffixes\n",
    "    x = x.replace(\"- MCC\", \"\").replace(\"- CC\", \"\")\n",
    "    x = x.replace(\"MCC\", \"\").replace(\"CC\", \"\")\n",
    "    \n",
    "    # Remove leftover hyphens\n",
    "    x = x.replace(\"-\", \"\")\n",
    "    \n",
    "    return x\n",
    "df[\"B_PRIMDX_Clean\"] = df[\"B_PRIMDX\"].apply(clean_dx_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "442fae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "df['InitialDeterminationStatus_Flag'] = (\n",
    "    df['InitialDeterminationStatus']\n",
    "    .str.strip()\n",
    "    .str.upper()\n",
    "    .map({\n",
    "        'APPROVED': 0,\n",
    "        'DENIED': 1\n",
    "    })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "88bebb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(72654, 61)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "required_cols = [\n",
    "    \"Payor\",\"claimid\",\"Client\",\"Control_ID\",\"Claim_Number\",\"PaymentType\",\n",
    "    \"Group_Type\",\"Payments\",\"Tax_ID\",\"Provider_ID\",\"HospitalName\",\"LOS\",\n",
    "    \"DISP_Code\",\"ADRG\",\"DischargeYear\",\"DischargeMonth\",\"AGE\",\n",
    "    \"A_CCCount\",\"A_MCCCount\",\"B_CCCount\",\"B_MCCCount\",\"LOB\",\n",
    "    \"MRREQ\",\"MRREC\",\"PRIM_DX\",\n",
    "    \"B_DISPCode\",\"B_DenialCode\",\"B_DRG\",\"B_PRIMDX\",\n",
    "    \"InitialDeterminationDate\",\"InitialDenialType\",\n",
    "    \"InitialDeterminationStatus\",\"IDSavings\",\"AdjustedSavings\",\n",
    "    \"DXRemoved_AtoB\",\"PRIMDX_DownCoded\",\n",
    "    \"InitialCoder\",\"InitialCoder_Employer\",\"InitialCoderID\",\n",
    "    \"InitialDoctor\",\"InitialDoctorID\",\n",
    "    \"InitialProcessor\",\"InitialProcessorID\",\n",
    "    \"len_adrg\",\"InitialDeterminationDate_copy\",\n",
    "    \"InitialDeterminationDate_date\",\"InitialDetermination_Year\",\n",
    "    \"A_DX_List\",\"B_DX_List\",\n",
    "    \"A_DX_List_Clean\",\"B_DX_List_Clean\",\n",
    "    \"A_DX_Type_list\",\"B_DX_Type_list\",\n",
    "    \"A_DX_Type\",\"A_DX_Set\",\n",
    "    \"B_DX_Type\",\"B_DX_Set\",\n",
    "    \"A_Proc_List\",\"B_Proc_List\",\n",
    "    \"B_PRIMDX_Clean\",\"InitialDeterminationStatus_Flag\"\n",
    "]\n",
    "\n",
    "df_filtered = df[required_cols]\n",
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cac576e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_excel(\"20260219 Humana Data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418a138",
   "metadata": {},
   "source": [
    "# Post Pay + Pre Pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac41b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "stats = defaultdict(lambda: {\n",
    "    'Total_Claims': 0,\n",
    "    'Approved': 0,\n",
    "    'Denied': 0,\n",
    "    'Total_Savings': 0.0\n",
    "})\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "\n",
    "    drg = row['ADRG']\n",
    "    pdx = row['PRIM_DX']\n",
    "    saving = row['IDSavings']\n",
    "    audit_result = row['InitialDeterminationStatus_Flag']\n",
    "    status = 'APPROVED' if audit_result == 0 else 'DENIED'\n",
    "  \n",
    "    group_key = (drg,pdx)\n",
    "\n",
    "    # --- Update stats ---\n",
    "    stats[group_key]['Total_Claims'] += 1\n",
    "   \n",
    "    if status == 'APPROVED':\n",
    "        stats[group_key]['Approved'] += 1\n",
    "    else:\n",
    "        stats[group_key]['Denied'] += 1\n",
    "        if audit_result == 1:\n",
    "            stats[group_key]['Total_Savings'] += float(saving)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Build Final DataFrame\n",
    "# -----------------------------\n",
    "final_data = []\n",
    "\n",
    "for key, data in stats.items():\n",
    "    drg,pdx = key   # ✅ FIXED unpacking\n",
    "\n",
    "    total_claims = data['Total_Claims']\n",
    "    approved = data['Approved']\n",
    "    denied = data['Denied']\n",
    "    total_savings = round(data['Total_Savings'], 2)\n",
    "    approval_percent = round((approved / total_claims) * 100, 2) if total_claims else 0\n",
    "    denial_percent = round((denied / total_claims) * 100, 2) if total_claims else 0\n",
    "    avg_saving_per_claim = round(total_savings / total_claims, 2) if total_claims else 0\n",
    "    avg_saving_per_den_claim = round(total_savings / denied, 2) if denied else 0\n",
    "\n",
    "\n",
    "\n",
    "    final_data.append({\n",
    "        'DRG': drg,\n",
    "        'PDX': pdx,\n",
    "        'Total_Claims': total_claims,\n",
    "        'Approved_Claims': approved,\n",
    "        'Approval_Percent': approval_percent,\n",
    "        'Denied_Claims': denied,\n",
    "        'Denial_Percent': denial_percent,\n",
    "        'Total_Savings': total_savings,\n",
    "        'Avg_Saving_per_Claim': avg_saving_per_claim,\n",
    "        'Avg_Saving_per_Denied_Claim': avg_saving_per_den_claim\n",
    "\n",
    "    })\n",
    "\n",
    "df_com = pd.DataFrame(final_data)\n",
    "\n",
    "df_com['Referral_%'] = (((30 * df_com['Denial_Percent'])/70) + df_com['Denial_Percent'])/100\n",
    "df_com['Phy_Cost'] = df_com['Referral_%'] * df_com['Total_Claims'] * 50\n",
    "df_com['Coder_Cost'] = df_com['Total_Claims'] * 12\n",
    "df_com['Expense'] = df_com['Phy_Cost'] + df_com['Coder_Cost']\n",
    "df_com['Expense_Per_Claim'] = df_com['Expense'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Commission'] = df_com['Total_Savings'] * 0.17\n",
    "df_com['Commission_Per_Claim'] = df_com['Commission'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Net_Profit'] = df_com['Commission'] - df_com['Expense']\n",
    "df_com['Net_Profit_Per_Claim'] = df_com['Net_Profit']/ df_com['Total_Claims']\n",
    "\n",
    "df_com.to_excel(\"ALL Humnana DRG+PDX Denial Analysis.xlsx\")\n",
    "\n",
    "required_columns = [\n",
    "    'DRG',\n",
    "    'PDX',\n",
    "    'Total_Claims',\n",
    "    'Denied_Claims',\n",
    "    'Denial_Percent',\n",
    "    'Total_Savings',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Expense_Per_Claim',\n",
    "    'Commission_Per_Claim',\n",
    "    'Net_Profit_Per_Claim'\n",
    "]\n",
    "\n",
    "df_com = df_com[required_columns]\n",
    "df_com.head()\n",
    "\n",
    "\n",
    "# Convert required columns to numeric\n",
    "numeric_cols = [\n",
    "    'Denial_Percent',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Total_Claims'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_com[col] = pd.to_numeric(df_com[col], errors='coerce')\n",
    "\n",
    "# Base condition\n",
    "base_condition = df_com['Total_Claims'] >= 5\n",
    "\n",
    "# ---------------------------\n",
    "# 4 Combination DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "HighDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "LowDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "HighDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "LowDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "LowDen_LowSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 250)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Remaining Claim Details (> 5 Claims)\n",
    "# ---------------------------\n",
    "\n",
    "Remaining_Claim_Details = df_com[\n",
    "    df_com['Total_Claims'] < 5\n",
    "]\n",
    "\n",
    "# Function to calculate summary\n",
    "def create_summary(df, label):\n",
    "    \n",
    "    unique_combo = df[['DRG', 'PDX']].drop_duplicates().shape[0]\n",
    "    total_claims = df['Total_Claims'].sum()\n",
    "    denied_claims = df['Denied_Claims'].sum()\n",
    "    denial_percent = (denied_claims / total_claims * 100) if total_claims != 0 else 0\n",
    "    total_saving = df['Total_Savings'].sum()\n",
    "    \n",
    "    return {\n",
    "        'Category': label,\n",
    "        'Unique Combination (DRG+PDX)': unique_combo,\n",
    "        'Claims': total_claims,\n",
    "        'Denied Claims': denied_claims,\n",
    "        'Denied %': round(denial_percent, 2),\n",
    "        'Total Saving': round(total_saving, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Summary Data\n",
    "summary_data = [\n",
    "    create_summary(HighDen_HighSaving, \"High Den % & High Saving per Claim\"),\n",
    "    create_summary(LowDen_HighSaving, \"Low Den % & High Saving per Claim\"),\n",
    "    create_summary(HighDen_MidSaving, \"High Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(LowDen_MidSaving, \"Low Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(LowDen_LowSaving, \"Low Den % & Low Saving per Claim\"),\n",
    "    create_summary(Remaining_Claim_Details, \"Remainung Claims < 5\")\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "df_summary.head()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Export to Excel\n",
    "# ---------------------------\n",
    "\n",
    "with pd.ExcelWriter(\"DRG_PDX_Combination_Denial_Analysis.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name=\"Insights\", index=False)\n",
    "    HighDen_HighSaving.to_excel(writer, sheet_name=\"HighDen_HighSaving\", index=False)\n",
    "    LowDen_HighSaving.to_excel(writer, sheet_name=\"LowDen_HighSaving\", index=False)\n",
    "    HighDen_MidSaving.to_excel(writer, sheet_name=\"HighDen_MidSaving\", index=False)\n",
    "    LowDen_MidSaving.to_excel(writer, sheet_name=\"LowDen_MidSaving\", index=False)\n",
    "    LowDen_LowSaving.to_excel(writer, sheet_name=\"LowDen_LowSaving\", index=False)\n",
    "    Remaining_Claim_Details.to_excel(writer, sheet_name=\"Remaining_Claims_<5\", index=False)\n",
    "\n",
    "print(\"Excel file created successfully ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c186df7",
   "metadata": {},
   "source": [
    "# Post pay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dcbcf583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PaymentType\n",
       "PostPay    64003\n",
       "Prepay      8651\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['PaymentType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "50f0e57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64003, 309)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ms_drg_postpay =df[df['PaymentType'] == 'PostPay']\n",
    "df_ms_drg_postpay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d161eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created successfully ✅\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "stats = defaultdict(lambda: {\n",
    "    'Total_Claims': 0,\n",
    "    'Approved': 0,\n",
    "    'Denied': 0,\n",
    "    'Total_Savings': 0.0\n",
    "})\n",
    "\n",
    "for idx, row in df_ms_drg_postpay.iterrows():\n",
    "\n",
    "    drg = row['ADRG']\n",
    "    pdx = row['PRIM_DX']\n",
    "    saving = row['IDSavings']\n",
    "    audit_result = row['InitialDeterminationStatus_Flag']\n",
    "    status = 'APPROVED' if audit_result == 0 else 'DENIED'\n",
    "  \n",
    "    group_key = (drg,pdx)\n",
    "\n",
    "    # --- Update stats ---\n",
    "    stats[group_key]['Total_Claims'] += 1\n",
    "   \n",
    "    if status == 'APPROVED':\n",
    "        stats[group_key]['Approved'] += 1\n",
    "    else:\n",
    "        stats[group_key]['Denied'] += 1\n",
    "        if audit_result == 1:\n",
    "            stats[group_key]['Total_Savings'] += float(saving)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Build Final DataFrame\n",
    "# -----------------------------\n",
    "final_data = []\n",
    "\n",
    "for key, data in stats.items():\n",
    "    drg,pdx = key   # ✅ FIXED unpacking\n",
    "\n",
    "    total_claims = data['Total_Claims']\n",
    "    approved = data['Approved']\n",
    "    denied = data['Denied']\n",
    "    total_savings = round(data['Total_Savings'], 2)\n",
    "    approval_percent = round((approved / total_claims) * 100, 2) if total_claims else 0\n",
    "    denial_percent = round((denied / total_claims) * 100, 2) if total_claims else 0\n",
    "    avg_saving_per_claim = round(total_savings / total_claims, 2) if total_claims else 0\n",
    "    avg_saving_per_den_claim = round(total_savings / denied, 2) if denied else 0\n",
    "\n",
    "\n",
    "\n",
    "    final_data.append({\n",
    "        'DRG': drg,\n",
    "        'PDX': pdx,\n",
    "        'Total_Claims': total_claims,\n",
    "        'Approved_Claims': approved,\n",
    "        'Approval_Percent': approval_percent,\n",
    "        'Denied_Claims': denied,\n",
    "        'Denial_Percent': denial_percent,\n",
    "        'Total_Savings': total_savings,\n",
    "        'Avg_Saving_per_Claim': avg_saving_per_claim,\n",
    "        'Avg_Saving_per_Denied_Claim': avg_saving_per_den_claim\n",
    "\n",
    "    })\n",
    "\n",
    "df_com = pd.DataFrame(final_data)\n",
    "\n",
    "df_com['Referral_%'] = (((30 * df_com['Denial_Percent'])/70) + df_com['Denial_Percent'])/100\n",
    "df_com['Phy_Cost'] = df_com['Referral_%'] * df_com['Total_Claims'] * 50\n",
    "df_com['Coder_Cost'] = df_com['Total_Claims'] * 12\n",
    "df_com['Expense'] = df_com['Phy_Cost'] + df_com['Coder_Cost']\n",
    "df_com['Expense_Per_Claim'] = df_com['Expense'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Commission'] = df_com['Total_Savings'] * 0.17\n",
    "df_com['Commission_Per_Claim'] = df_com['Commission'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Net_Profit'] = df_com['Commission'] - df_com['Expense']\n",
    "df_com['Net_Profit_Per_Claim'] = df_com['Net_Profit']/ df_com['Total_Claims']\n",
    "\n",
    "df_com.head()\n",
    "\n",
    "\n",
    "required_columns = [\n",
    "    'DRG',\n",
    "    'PDX',\n",
    "    'Total_Claims',\n",
    "    'Denied_Claims',\n",
    "    'Denial_Percent',\n",
    "    'Total_Savings',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Expense_Per_Claim',\n",
    "    'Commission_Per_Claim',\n",
    "    'Net_Profit_Per_Claim'\n",
    "]\n",
    "\n",
    "df_com = df_com[required_columns]\n",
    "df_com.head()\n",
    "\n",
    "df_com.to_excel(\"PostPay Humnana DRG+PDX Denial Analysis.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# Convert required columns to numeric\n",
    "numeric_cols = [\n",
    "    'Denial_Percent',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Total_Claims'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_com[col] = pd.to_numeric(df_com[col], errors='coerce')\n",
    "\n",
    "# Base condition\n",
    "base_condition = df_com['Total_Claims'] >= 5\n",
    "\n",
    "# ---------------------------\n",
    "# 4 Combination DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "post_HighDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "post_LowDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "post_HighDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "post_LowDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "post_LowDen_LowSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 250)\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Remaining Claim Details (> 5 Claims)\n",
    "# ---------------------------\n",
    "\n",
    "post_Remaining_Claim_Details = df_com[\n",
    "    df_com['Total_Claims'] < 5\n",
    "]\n",
    "\n",
    "# Function to calculate summary\n",
    "def create_summary(df, label):\n",
    "    \n",
    "    unique_combo = df[['DRG', 'PDX']].drop_duplicates().shape[0]\n",
    "    total_claims = df['Total_Claims'].sum()\n",
    "    denied_claims = df['Denied_Claims'].sum()\n",
    "    denial_percent = (denied_claims / total_claims * 100) if total_claims != 0 else 0\n",
    "    total_saving = df['Total_Savings'].sum()\n",
    "    \n",
    "    return {\n",
    "        'Category': label,\n",
    "        'Unique Combination (DRG+PDX)': unique_combo,\n",
    "        'Claims': total_claims,\n",
    "        'Denied Claims': denied_claims,\n",
    "        'Denied %': round(denial_percent, 2),\n",
    "        'Total Saving': round(total_saving, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Summary Data\n",
    "summary_data = [\n",
    "    create_summary(post_HighDen_HighSaving, \"High Den % & High Saving per Claim\"),\n",
    "    create_summary(post_LowDen_HighSaving, \"Low Den % & High Saving per Claim\"),\n",
    "    create_summary(post_HighDen_MidSaving, \"High Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(post_LowDen_MidSaving, \"Low Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(post_LowDen_LowSaving, \"Low Den % & Low Saving per Claim\")\n",
    "    create_summary(post_Remaining_Claim_Details, \"Remainung Claims < 5\")\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_post_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "df_post_summary.head()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Export to Excel\n",
    "# ---------------------------\n",
    "\n",
    "with pd.ExcelWriter(\"Postpay DRG_PDX_Combination_Analysis 2024 2025.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_post_summary.to_excel(writer, sheet_name=\"post_Insights\", index=False)\n",
    "    post_HighDen_HighSaving.to_excel(writer, sheet_name=\"post_HighDen_HighSaving\", index=False)\n",
    "    post_LowDen_HighSaving.to_excel(writer, sheet_name=\"post_LowDen_HighSaving\", index=False)\n",
    "    post_HighDen_MidSaving.to_excel(writer, sheet_name=\"post_HighDen_MidSaving\", index=False)\n",
    "    post_LowDen_MidSaving.to_excel(writer, sheet_name=\"post_LowDen_MidSaving\", index=False)\n",
    "    post_LowDen_LowSaving.to_excel(writer, sheet_name=\"post_LowDen_LowSaving\", index=False)\n",
    "    post_Remaining_Claim_Details.to_excel(writer, sheet_name=\"post_Remaining_Claims_<5\", index=False)\n",
    "\n",
    "print(\"Excel file created successfully ✅\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e99f4",
   "metadata": {},
   "source": [
    "# prepay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d065914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8651, 309)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ms_drg_prepay =df[df['PaymentType'] == 'Prepay']\n",
    "df_ms_drg_prepay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bce821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created successfully ✅\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "stats = defaultdict(lambda: {\n",
    "    'Total_Claims': 0,\n",
    "    'Approved': 0,\n",
    "    'Denied': 0,\n",
    "    'Total_Savings': 0.0\n",
    "})\n",
    "\n",
    "for idx, row in df_ms_drg_prepay.iterrows():\n",
    "\n",
    "    drg = row['ADRG']\n",
    "    pdx = row['PRIM_DX']\n",
    "    saving = row['IDSavings']\n",
    "    audit_result = row['InitialDeterminationStatus_Flag']\n",
    "    status = 'APPROVED' if audit_result == 0 else 'DENIED'\n",
    "  \n",
    "    group_key = (drg,pdx)\n",
    "\n",
    "    # --- Update stats ---\n",
    "    stats[group_key]['Total_Claims'] += 1\n",
    "   \n",
    "    if status == 'APPROVED':\n",
    "        stats[group_key]['Approved'] += 1\n",
    "    else:\n",
    "        stats[group_key]['Denied'] += 1\n",
    "        if audit_result == 1:\n",
    "            stats[group_key]['Total_Savings'] += float(saving)\n",
    "\n",
    "# -----------------------------\n",
    "# 7. Build Final DataFrame\n",
    "# -----------------------------\n",
    "final_data = []\n",
    "\n",
    "for key, data in stats.items():\n",
    "    drg,pdx = key   # ✅ FIXED unpacking\n",
    "\n",
    "    total_claims = data['Total_Claims']\n",
    "    approved = data['Approved']\n",
    "    denied = data['Denied']\n",
    "    total_savings = round(data['Total_Savings'], 2)\n",
    "    approval_percent = round((approved / total_claims) * 100, 2) if total_claims else 0\n",
    "    denial_percent = round((denied / total_claims) * 100, 2) if total_claims else 0\n",
    "    avg_saving_per_claim = round(total_savings / total_claims, 2) if total_claims else 0\n",
    "    avg_saving_per_den_claim = round(total_savings / denied, 2) if denied else 0\n",
    "\n",
    "\n",
    "\n",
    "    final_data.append({\n",
    "        'DRG': drg,\n",
    "        'PDX': pdx,\n",
    "        'Total_Claims': total_claims,\n",
    "        'Approved_Claims': approved,\n",
    "        'Approval_Percent': approval_percent,\n",
    "        'Denied_Claims': denied,\n",
    "        'Denial_Percent': denial_percent,\n",
    "        'Total_Savings': total_savings,\n",
    "        'Avg_Saving_per_Claim': avg_saving_per_claim,\n",
    "        'Avg_Saving_per_Denied_Claim': avg_saving_per_den_claim\n",
    "\n",
    "    })\n",
    "\n",
    "df_com = pd.DataFrame(final_data)\n",
    "\n",
    "df_com['Referral_%'] = (((30 * df_com['Denial_Percent'])/70) + df_com['Denial_Percent'])/100\n",
    "df_com['Phy_Cost'] = df_com['Referral_%'] * df_com['Total_Claims'] * 50\n",
    "df_com['Coder_Cost'] = df_com['Total_Claims'] * 12\n",
    "df_com['Expense'] = df_com['Phy_Cost'] + df_com['Coder_Cost']\n",
    "df_com['Expense_Per_Claim'] = df_com['Expense'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Commission'] = df_com['Total_Savings'] * 0.17\n",
    "df_com['Commission_Per_Claim'] = df_com['Commission'] / df_com['Total_Claims']\n",
    "\n",
    "df_com['Net_Profit'] = df_com['Commission'] - df_com['Expense']\n",
    "df_com['Net_Profit_Per_Claim'] = df_com['Net_Profit']/ df_com['Total_Claims']\n",
    "\n",
    "df_com.head()\n",
    "\n",
    "\n",
    "required_columns = [\n",
    "    'DRG',\n",
    "    'PDX',\n",
    "    'Total_Claims',\n",
    "    'Denied_Claims',\n",
    "    'Denial_Percent',\n",
    "    'Total_Savings',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Expense_Per_Claim',\n",
    "    'Commission_Per_Claim',\n",
    "    'Net_Profit_Per_Claim'\n",
    "]\n",
    "\n",
    "df_com = df_com[required_columns]\n",
    "df_com.head()\n",
    "\n",
    "\n",
    "df_com.to_excel(\"PrePay Humnana DRG+PDX Denial Analysis.xlsx\")\n",
    "\n",
    "# Convert required columns to numeric\n",
    "numeric_cols = [\n",
    "    'Denial_Percent',\n",
    "    'Avg_Saving_per_Claim',\n",
    "    'Total_Claims'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df_com[col] = pd.to_numeric(df_com[col], errors='coerce')\n",
    "\n",
    "# Base condition\n",
    "base_condition = df_com['Total_Claims'] >= 5\n",
    "\n",
    "# ---------------------------\n",
    "# 4 Combination DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "pre_HighDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "pre_LowDen_HighSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 1000)\n",
    "]\n",
    "\n",
    "pre_HighDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] >= 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "pre_LowDen_MidSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] >= 250) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 1000)\n",
    "]\n",
    "\n",
    "pre_LowDen_LowSaving = df_com[\n",
    "    base_condition &\n",
    "    (df_com['Denial_Percent'] < 30) &\n",
    "    (df_com['Avg_Saving_per_Claim'] < 250)\n",
    "]\n",
    "\n",
    "# ---------------------------\n",
    "# Remaining Claim Details (> 5 Claims)\n",
    "# ---------------------------\n",
    "\n",
    "pre_Remaining_Claim_Details = df_com[\n",
    "    df_com['Total_Claims'] < 5\n",
    "]\n",
    "\n",
    "# Function to calculate summary\n",
    "def create_summary(df, label):\n",
    "    \n",
    "    unique_combo = df[['DRG', 'PDX']].drop_duplicates().shape[0]\n",
    "    total_claims = df['Total_Claims'].sum()\n",
    "    denied_claims = df['Denied_Claims'].sum()\n",
    "    denial_percent = (denied_claims / total_claims * 100) if total_claims != 0 else 0\n",
    "    total_saving = df['Total_Savings'].sum()\n",
    "    \n",
    "    return {\n",
    "        'Category': label,\n",
    "        'Unique Combination (DRG+PDX)': unique_combo,\n",
    "        'Claims': total_claims,\n",
    "        'Denied Claims': denied_claims,\n",
    "        'Denied %': round(denial_percent, 2),\n",
    "        'Total Saving': round(total_saving, 2)\n",
    "    }\n",
    "\n",
    "\n",
    "# Create Summary Data\n",
    "summary_data = [\n",
    "    create_summary(pre_HighDen_HighSaving, \"High Den % & High Saving per Claim\"),\n",
    "    create_summary(pre_LowDen_HighSaving, \"Low Den % & High Saving per Claim\"),\n",
    "    create_summary(pre_HighDen_MidSaving, \"High Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(pre_LowDen_MidSaving, \"Low Den % & Saving per Claim B/W $250 to $1000\"),\n",
    "    create_summary(pre_LowDen_LowSaving, \"Low Den % & Low Saving per Claim\"),\n",
    "    create_summary(pre_Remaining_Claim_Details, \"pre_Remaining_Claims_<5\")\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_pre_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "df_pre_summary.head()\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Export to Excel\n",
    "# ---------------------------\n",
    "\n",
    "with pd.ExcelWriter(\"Prepay DRG_PDX_Combination_Analysis 2024 2025.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_pre_summary.to_excel(writer, sheet_name=\"pre_Insights\", index=False)\n",
    "    pre_HighDen_HighSaving.to_excel(writer, sheet_name=\"pre_HighDen_HighSaving\", index=False)\n",
    "    pre_LowDen_HighSaving.to_excel(writer, sheet_name=\"pre_LowDen_HighSaving\", index=False)\n",
    "    pre_HighDen_MidSaving.to_excel(writer, sheet_name=\"pre_HighDen_MidSaving\", index=False)\n",
    "    pre_LowDen_MidSaving.to_excel(writer, sheet_name=\"pre_LowDen_MidSaving\", index=False)\n",
    "    pre_LowDen_LowSaving.to_excel(writer, sheet_name=\"pre_LowDen_LowSaving\", index=False)\n",
    "    pre_Remaining_Claim_Details.to_excel(writer, sheet_name=\"pre_Remaining_Claims_<5\", index=False)\n",
    "\n",
    "print(\"Excel file created successfully ✅\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57fbdc7",
   "metadata": {},
   "source": [
    "# all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter(\"DRG_PDX_Combination_Analysis 2024 2025.xlsx\", engine=\"openpyxl\") as writer:\n",
    "    df_summary.to_excel(writer, sheet_name=\"Insights\", index=False)\n",
    "    HighDen_HighSaving.to_excel(writer, sheet_name=\"HighDen_HighSaving\", index=False)\n",
    "    LowDen_HighSaving.to_excel(writer, sheet_name=\"LowDen_HighSaving\", index=False)\n",
    "    HighDen_MidSaving.to_excel(writer, sheet_name=\"HighDen_MidSaving\", index=False)\n",
    "    LowDen_MidSaving.to_excel(writer, sheet_name=\"LowDen_MidSaving\", index=False)\n",
    "    LowDen_LowSaving.to_excel(writer, sheet_name=\"LowDen_LowSaving\", index=False)\n",
    "    Remaining_Claim_Details.to_excel(writer, sheet_name=\"Remaining_Claims_<5\", index=False)\n",
    "\n",
    "    df_post_summary.to_excel(writer, sheet_name=\"post_Insights\", index=False)\n",
    "    post_HighDen_HighSaving.to_excel(writer, sheet_name=\"post_HighDen_HighSaving\", index=False)\n",
    "    post_LowDen_HighSaving.to_excel(writer, sheet_name=\"post_LowDen_HighSaving\", index=False)\n",
    "    post_HighDen_MidSaving.to_excel(writer, sheet_name=\"post_HighDen_MidSaving\", index=False)\n",
    "    post_LowDen_MidSaving.to_excel(writer, sheet_name=\"post_LowDen_MidSaving\", index=False)\n",
    "    post_LowDen_LowSaving.to_excel(writer, sheet_name=\"post_LowDen_LowSaving\", index=False)\n",
    "    post_Remaining_Claim_Details.to_excel(writer, sheet_name=\"post_Remaining_Claims_<5\", index=False)\n",
    "\n",
    "    df_pre_summary.to_excel(writer, sheet_name=\"pre_Insights\", index=False)\n",
    "    pre_HighDen_HighSaving.to_excel(writer, sheet_name=\"pre_HighDen_HighSaving\", index=False)\n",
    "    pre_LowDen_HighSaving.to_excel(writer, sheet_name=\"pre_LowDen_HighSaving\", index=False)\n",
    "    pre_HighDen_MidSaving.to_excel(writer, sheet_name=\"pre_HighDen_MidSaving\", index=False)\n",
    "    pre_LowDen_MidSaving.to_excel(writer, sheet_name=\"pre_LowDen_MidSaving\", index=False)\n",
    "    pre_LowDen_LowSaving.to_excel(writer, sheet_name=\"pre_LowDen_LowSaving\", index=False)\n",
    "    pre_Remaining_Claim_Details.to_excel(writer, sheet_name=\"pre_Remaining_Claims_<5\", index=False)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
